# Triton Inference Server Configuration
# Copy this file to .env and modify as needed

# Server Configuration
TRITON_SERVER_URL=localhost:8001
TRITON_HTTP_PORT=8000
TRITON_GRPC_PORT=8001
TRITON_METRICS_PORT=8002

# Model Repository Configuration
MODEL_REPOSITORY=./rbln_backend
MODEL_NAME=yolov11
BACKEND_TYPE=rbln

# Backend Type Options:
# - rbln: NPU backend (Rebellions RBLN)
# - gpu: GPU backend (PyTorch)

# Logging Configuration
LOG_VERBOSE=1

# Model Configuration
CONFIDENCE_THRESHOLD=0.20
IOU_THRESHOLD=0.50
MAX_DETECTIONS=1024

# Client Configuration
# Python Client
PYTHON_CLIENT_TIMEOUT=30.0
PYTHON_CLIENT_VERBOSE=false

# C++ Client
CPP_CLIENT_WORKER_THREADS=4
CPP_CLIENT_BATCH_SIZE=1

# Performance Configuration
MAX_ASYNC_REQUESTS=16
REQUEST_TIMEOUT=10.0

# Video Processing Configuration
VIDEO_FPS=24.0
VIDEO_MAX_HISTORY=2
VIDEO_DISTANCE_THRESHOLD=50
VIDEO_LINE_THICKNESS=4
VIDEO_FONT_SCALE=0.6
VIDEO_FONT_THICKNESS=2

# JPEG Quality (for image encoding)
JPEG_QUALITY=80

# Output Configuration
OUTPUT_DIR=./output
MEDIA_DIR=./client/media

# Class Names Configuration
DATA_YAML=./rbln_backend/yolov11/1/coco128.yaml

# GPU Configuration (if using GPU backend)
CUDA_VISIBLE_DEVICES=0

# NPU Configuration (if using NPU backend)
# RBLN_DEVICE_ID=0

# Optional: Custom paths
# PYTHON_PATH=/usr/bin/python3
# TRITON_SERVER_PATH=/usr/bin/tritonserver
