# C++ í´ë¼ì´ì–¸íŠ¸ ì•„í‚¤í…ì²˜ ìƒì„¸ ë¶„ì„

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    A[Main Thread] --> B[Thread Pool]
    B --> C[Worker Thread 1]
    B --> D[Worker Thread 2]
    B --> E[Worker Thread N]
    
    C --> F[gRPC Stub]
    D --> F
    E --> F
    
    F --> G[Triton Server]
    
    H[Task Queue] --> B
    I[Performance Tracker] --> A
    
    A --> J[Rate Controller]
    J --> H
```

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. TritonCppClient í´ë˜ìŠ¤

```cpp
class TritonCppClient {
private:
    // gRPC ì—°ê²°
    std::unique_ptr<GRPCInferenceService::Stub> stub_;
    
    // ëª¨ë¸ ì„¤ì •
    std::string model_name_;
    int input_width_, input_height_;
    
    // ì„±ëŠ¥ ì¶”ì  (ì›ìì  ì—°ì‚°)
    std::atomic<int> total_requests_{0};
    std::atomic<double> total_inference_time_{0.0};
    std::atomic<double> total_e2e_time_{0.0};
    std::mutex stats_mutex_;
    
    // ìŠ¤ë ˆë“œ í’€ ê´€ë¦¬
    std::vector<std::thread> workers_;
    std::queue<std::function<void()>> task_queue_;
    std::mutex queue_mutex_;
    std::condition_variable queue_cv_;
    std::atomic<bool> stop_flag_{false};
    
    // ìš”ì²­ ì¶”ì 
    std::atomic<int> completed_requests_{0};
    std::atomic<int> target_requests_{0};
};
```

### 2. ìŠ¤ë ˆë“œ í’€ ì•„í‚¤í…ì²˜

```cpp
void start_workers() {
    int num_workers = std::thread::hardware_concurrency(); // CPU ì½”ì–´ ìˆ˜
    std::cout << "Starting " << num_workers << " worker threads" << std::endl;
    
    for (int i = 0; i < num_workers; ++i) {
        workers_.emplace_back([this, i]() {
            worker_thread(i);  // ê° ì›Œì»¤ê°€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰
        });
    }
}
```

### 3. ì›Œì»¤ ìŠ¤ë ˆë“œ ë™ì‘

```cpp
void worker_thread(int worker_id) {
    while (!stop_flag_) {
        std::function<void()> task;
        {
            std::unique_lock<std::mutex> lock(queue_mutex_);
            // ì‘ì—…ì´ ìˆê±°ë‚˜ ì¢…ë£Œ ì‹ í˜¸ê°€ ì˜¬ ë•Œê¹Œì§€ ëŒ€ê¸°
            queue_cv_.wait(lock, [this] { 
                return !task_queue_.empty() || stop_flag_; 
            });
            
            if (stop_flag_) break;
            
            if (!task_queue_.empty()) {
                task = task_queue_.front();
                task_queue_.pop();
            }
        }
        
        if (task) {
            task();  // ì‹¤ì œ ì¶”ë¡  ì‘ì—… ì‹¤í–‰
        }
    }
}
```

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

### 1. ìš”ì²­ ìƒì„± ë° ì „ì†¡

```mermaid
sequenceDiagram
    participant M as Main Thread
    participant Q as Task Queue
    participant W as Worker Thread
    participant G as gRPC Stub
    participant S as Triton Server
    
    M->>Q: add_task(inference_request)
    Q->>W: task available
    W->>G: ModelInfer(request)
    G->>S: gRPC call
    S-->>G: response
    G-->>W: result
    W->>M: update statistics
```

### 2. ì„±ëŠ¥ ì¸¡ì • í”Œë¡œìš°

```cpp
bool single_inference(int request_id) {
    auto start_time = std::chrono::high_resolution_clock::now();
    
    // 1. ë”ë¯¸ ë°ì´í„° ìƒì„±
    auto dummy_data = create_dummy_input();
    
    // 2. gRPC ìš”ì²­ êµ¬ì„±
    ModelInferRequest request;
    request.set_model_name(model_name_);
    // ... ì…ë ¥/ì¶œë ¥ ì„¤ì •
    
    // 3. ì¶”ë¡  ì‹¤í–‰
    auto inference_start = std::chrono::high_resolution_clock::now();
    Status status = stub_->ModelInfer(&context, request, &response);
    auto inference_end = std::chrono::high_resolution_clock::now();
    
    // 4. ì„±ëŠ¥ ì¸¡ì •
    auto inference_time = std::chrono::duration<double, std::milli>(
        inference_end - inference_start).count();
    auto e2e_time = std::chrono::duration<double, std::milli>(
        end_time - start_time).count();
    
    // 5. í†µê³„ ì—…ë°ì´íŠ¸ (ìŠ¤ë ˆë“œ ì•ˆì „)
    {
        std::lock_guard<std::mutex> lock(stats_mutex_);
        total_inference_time_ = total_inference_time_.load() + inference_time;
        total_e2e_time_ = total_e2e_time_.load() + e2e_time;
    }
    total_requests_++;
    completed_requests_++;
}
```

## âš¡ ì„±ëŠ¥ ìµœì í™” ê¸°ë²•

### 1. ë©”ëª¨ë¦¬ ìµœì í™”

```cpp
// ë”ë¯¸ ë°ì´í„° ìƒì„± (ëœë¤ ê°’)
std::vector<float> create_dummy_input() {
    std::vector<float> data(input_width_ * input_height_ * 3);
    
    static std::random_device rd;
    static std::mt19937 gen(rd());
    static std::uniform_real_distribution<float> dis(0.0f, 1.0f);
    
    for (auto& val : data) {
        val = dis(gen);
    }
    
    return data;
}
```

### 2. ìš”ì²­ ì†ë„ ì œì–´

```cpp
void run_performance_test(int num_requests, int request_rate) {
    auto start_time = std::chrono::high_resolution_clock::now();
    
    for (int i = 0; i < num_requests; ++i) {
        // ì‘ì—… íì— ì¶”ê°€
        add_task([this, i]() { single_inference(i); });
        
        // ìš”ì²­ ì†ë„ ì œì–´ (ì •í™•í•œ íƒ€ì´ë°)
        auto elapsed = std::chrono::high_resolution_clock::now() - start_time;
        auto target_time = std::chrono::milliseconds(i * 1000 / request_rate);
        if (elapsed < target_time) {
            std::this_thread::sleep_for(target_time - elapsed);
        }
    }
}
```

### 3. ìŠ¤ë ˆë“œ ë™ê¸°í™”

```cpp
void add_task(std::function<void()> task) {
    {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        task_queue_.push(task);
    }
    queue_cv_.notify_one();  // ëŒ€ê¸° ì¤‘ì¸ ì›Œì»¤ì—ê²Œ ì•Œë¦¼
}
```

## ğŸ” ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§

### 1. ì§„í–‰ ìƒí™© ì¶œë ¥

```cpp
if (completed_requests_ % 100 == 0) {
    double current_fps = completed_requests_.load() / 
        (std::chrono::duration<double>(end_time - start_time).count());
    std::cout << "Processed " << completed_requests_ << "/" << target_requests_ 
             << " requests | Inference: " << inference_time << "ms | FPS: " << current_fps << std::endl;
}
```

### 2. ìµœì¢… í†µê³„

```cpp
// ìµœì¢… ì„±ëŠ¥ í†µê³„ ê³„ì‚°
double avg_fps = total_requests_ / total_time;
double avg_inference = total_inference_time_ / total_requests_;
double avg_e2e = total_e2e_time_ / total_requests_;

std::cout << "C++ CLIENT PERFORMANCE RESULTS" << std::endl;
std::cout << "Total requests: " << total_requests_ << std::endl;
std::cout << "Average FPS: " << avg_fps << std::endl;
std::cout << "Average inference time: " << avg_inference << "ms" << std::endl;
std::cout << "Average E2E time: " << avg_e2e << "ms" << std::endl;
```

## ğŸš€ ë¹Œë“œ ìµœì í™”

### CMakeLists.txt ìµœì í™”

```cmake
# ì»´íŒŒì¼ëŸ¬ ìµœì í™”
target_compile_options(triton_cpp_client PRIVATE
    -O3                    # ìµœê³  ìˆ˜ì¤€ ìµœì í™”
    -march=native          # CPU íŠ¹í™” ìµœì í™”
    -mtune=native          # CPU íŠœë‹
    -flto                  # Link Time Optimization
)

# ë§í¬ íƒ€ì„ ìµœì í™”
set_target_properties(triton_cpp_client PROPERTIES
    LINK_FLAGS "-flto"
)
```

### ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# Release ëª¨ë“œë¡œ ë¹Œë“œ
cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_CXX_FLAGS="-O3 -march=native -mtune=native" \
    -DCMAKE_EXE_LINKER_FLAGS="-flto"

# ë³‘ë ¬ ì»´íŒŒì¼
make -j$(nproc)
```

## ğŸ“Š ì„±ëŠ¥ ë¶„ì„

### 1. ë³‘ëª© ì§€ì  ì‹ë³„

| êµ¬ì„± ìš”ì†Œ | ì‹œê°„ | ë¹„ìœ¨ |
|-----------|------|------|
| **Inference** | 28.6ms | 81% |
| **Network** | 4.2ms | 12% |
| **Processing** | 2.4ms | 7% |
| **Total E2E** | 35.2ms | 100% |

### 2. ìŠ¤ë ˆë“œ íš¨ìœ¨ì„±

```cpp
// CPU ì½”ì–´ ìˆ˜ë§Œí¼ ì›Œì»¤ ìƒì„±
int num_workers = std::thread::hardware_concurrency();
// â†’ 128ê°œ ì›Œì»¤ (ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¤ë¦„)
```

### 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

- **ìŠ¤íƒ ë©”ëª¨ë¦¬**: ê° ìŠ¤ë ˆë“œë‹¹ ~8MB
- **í™ ë©”ëª¨ë¦¬**: ë”ë¯¸ ë°ì´í„° + gRPC ë²„í¼
- **ì´ ë©”ëª¨ë¦¬**: ~1GB (128 ìŠ¤ë ˆë“œ ê¸°ì¤€)

## ğŸ¯ ìµœì í™” ê²°ê³¼

### ì„±ëŠ¥ í–¥ìƒ ë‹¨ê³„

1. **ê¸°ë³¸ Python**: 64.9 FPS
2. **Python ìµœì í™”**: 75.0 FPS (+15.6%)
3. **C++ ë„¤ì´í‹°ë¸Œ**: **89.8 FPS** (+38.4%)

### í•µì‹¬ ì„±ê³µ ìš”ì¸

1. **Python GIL ì œê±°**: ì§„ì •í•œ ë©€í‹°ìŠ¤ë ˆë”©
2. **ë„¤ì´í‹°ë¸Œ ì„±ëŠ¥**: ì»´íŒŒì¼ëŸ¬ ìµœì í™”
3. **íš¨ìœ¨ì  ë™ê¸°í™”**: ì›ìì  ì—°ì‚° + ë®¤í…ìŠ¤
4. **gRPC ìµœì í™”**: ë°”ì´ë„ˆë¦¬ í”„ë¡œí† ì½œ

---

**C++ í´ë¼ì´ì–¸íŠ¸**ëŠ” **90fps ëª©í‘œì— 0.2fpsë§Œ ë¶€ì¡±**í•œ **ìµœê³  ì„±ëŠ¥**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤! ğŸš€
